[sources.vector_metrics]
type = "internal_metrics"

[sinks.vector_exporter]
type = "prometheus_exporter"
inputs = ["vector_metrics"]
address = "${VECTOR_PROMETHUS_ADDR}"
default_namespace = "vector"
suppress_timestamp = true

# nginx

[sources.nginx]
type = "journald"
include_units = ["nginx"]

# nginx access

[transforms.nginx_access]
type = "filter"
inputs = ["nginx"]
condition = ".SYSLOG_IDENTIFIER == \"access\""

[transforms.nginx_access_preprocessed]
type = "remap"
inputs = ["nginx_access"]
source = """
. = parse_json!(.message)

# Anonymize remote_addr
remote_addr, err = to_string(.remote_addr)
if err != null || remote_addr == "" {
  .remote_addr_family = 0;
  .remote_addr_hashed = ""
} else {
  # Make sure it's properly parsed as either IPv4 or IPv6, otherwise fall back to 0
  .remote_addr_family = if is_ipv4(remote_addr) { 4 } else if is_ipv6(remote_addr) { 6 } else { 0 };

  .remote_addr_hashed = sha3("${IP_HASH_SALT}" + remote_addr, variant: "SHA3-224")
  .remote_addr_hashed = truncate(.remote_addr_hashed, limit: 32, ellipsis: false)
}

# Remove privacy related info
del(.remote_user)
del(.remote_addr)
del(.remote_port)

# parse status for later sampling
status, err = to_int(.status)
if err == null {
  .status = status
}

.ic_http_request =
  if .ic_method_name == "http_request" { 1 } else { 0 }
"""

[transforms.nginx_access_by_status]
type = "route"
inputs = ["nginx_access_preprocessed"]

  [transforms.nginx_access_by_status.route]
  1xx = '.status >= 100 && .status < 200 ?? false'
  2xx = '.status >= 200 && .status < 300 ?? false'
  3xx = '.status >= 300 && .status < 400 ?? false'
  4xx = '.status >= 400 && .status < 500 ?? false'
  5xx = '.status >= 500 && .status < 600 ?? false'

# nginx access (metrics)

[transforms.metrics_nginx_access_preprocessed]
type = "remap"
inputs = ["nginx_access_preprocessed"]
source = """
for_each([
  "ic_node_id",
  "ic_request_type",
  "ic_subnet_id",
]) -> |_, k| {
  if get!(., [k]) == "" {
    . = set!(., [k], "N/A")
  }
}
"""

[transforms.nginx_access_metrics]
type = "log_to_metric"
inputs = ["metrics_nginx_access_preprocessed"]

  [[transforms.nginx_access_metrics.metrics]]
  type = "counter"
  field = "status"
  name = "request_total"

    [transforms.nginx_access_metrics.metrics.tags]
    hostname = "{{ hostname }}"
    ic_http_request = "{{ ic_http_request }}"
    ic_node_id = "{{ ic_node_id }}"
    ic_request_type = "{{ ic_request_type }}"
    ic_subnet_id = "{{ ic_subnet_id }}"
    is_bot = "{{ is_bot }}"
    request_method = "{{ request_method }}"
    status = "{{ status }}"
    traffic_segment = "{{ traffic_segment }}"
    upstream_cache_status = "{{ upstream_cache_status }}"
    upstream_status = "{{ upstream_status }}"

  [[transforms.nginx_access_metrics.metrics]]
  type = "histogram"
  field = "request_time"
  name = "request_sec_duration"

    [transforms.nginx_access_metrics.metrics.tags]
    hostname = "{{ hostname }}"
    ic_http_request = "{{ ic_http_request }}"
    ic_node_id = "{{ ic_node_id }}"
    ic_request_type = "{{ ic_request_type }}"
    ic_subnet_id = "{{ ic_subnet_id }}"
    is_bot = "{{ is_bot }}"
    request_method = "{{ request_method }}"
    status = "{{ status }}"
    traffic_segment = "{{ traffic_segment }}"
    upstream_cache_status = "{{ upstream_cache_status }}"
    upstream_status = "{{ upstream_status }}"

# nginx access (clickhouse)

[transforms.clickhouse_nginx_access_2xx_sampled]
type = "sample"
inputs = ["nginx_access_by_status.2xx"]
rate = ${CLICKHOUSE_2XX_SAMPLE_RATE:?err}

[transforms.clickhouse_nginx_access_preprocessed]
type = "remap"
inputs = [
  "nginx_access_by_status.1xx",
  "clickhouse_nginx_access_2xx_sampled",
  "nginx_access_by_status.3xx",
  "nginx_access_by_status.4xx",
  "nginx_access_by_status.5xx",
  "nginx_access_by_status._unmatched",
]
source = """
# normalize https
.https = if .https == "on" { 1 } else { 0 }

# convert from float-second to int-ms
for_each([
  "connection_time",
  "request_time",
  "upstream_connect_time",
  "upstream_header_time",
  "upstream_response_time",
]) -> |_, k| {
  t_ms, err = to_float(get!(., [k])) * 1000
  if err != null {
    t_ms = null
  }

  . = set!(., [k + "_ms"], to_int(t_ms))
}

# rename field
.date = to_int(to_float!(.msec))
"""

[sinks.clickhouse_nginx_access]
type = "clickhouse"
inputs = ["clickhouse_nginx_access_preprocessed"]

# General
endpoint = "${CLICKHOUSE_URL}"
database = "default"
table = "http_access"

  [sinks.clickhouse_nginx_access.healthcheck]
  enabled = true

  [sinks.clickhouse_nginx_access.batch]
  max_bytes = 10485760 # 10 MB
  timeout_secs = 5

  [sinks.clickhouse_nginx_access.buffer]
  max_events = 10000 # 10 K
  type = "memory"
  when_full = "drop_newest"

  [sinks.clickhouse_nginx_access.request]
  retry_attempts = 3

  [sinks.clickhouse_nginx_access.auth]
  strategy = "basic"
  user = "${CLICKHOUSE_USER}"
  password = "${CLICKHOUSE_PASSWORD}"

  [sinks.clickhouse_nginx_access.encoding]
  only_fields = [
    "body_bytes_sent",
    "bytes_sent",
    "connection_time_ms",
    "content_length",
    "content_type",
    "date",
    "geo_city_name",
    "geo_country_code",
    "geo_country_name",
    "host",
    "hostname",
    "http_host",
    "http_origin",
    "http_referer",
    "http_user_agent",
    "https",
    "ic_canister_id",
    "ic_http_request",
    "ic_method_name",
    "ic_node_id",
    "ic_request_type",
    "ic_subnet_id",
    "is_bot",
    "nginx_version",
    "pre_isolation_canister",
    "proxy_host",
    "proxy_port",
    "query_string",
    "remote_addr_hashed",
    "remote_addr_family",
    "request_length",
    "request_method",
    "request_time_ms",
    "request_uri",
    "scheme",
    "server_addr",
    "server_name",
    "server_port",
    "server_protocol",
    "ssl_cipher",
    "ssl_protocol",
    "status",
    "traffic_segment",
    "upstream_addr",
    "upstream_bytes_received",
    "upstream_bytes_sent",
    "upstream_cache_status",
    "upstream_connect_time_ms",
    "upstream_header_time_ms",
    "upstream_response_length",
    "upstream_response_time_ms",
    "upstream_status"
  ]

# nginx access (elasticsearch)

[transforms.elasticsearch_nginx_access_2xx_sampled]
type = "sample"
inputs = ["nginx_access_by_status.2xx"]
rate = 100

[transforms.elasticsearch_nginx_access_preprocessed]
type = "remap"
inputs = [
  "nginx_access_by_status.1xx",
  "elasticsearch_nginx_access_2xx_sampled",
  "nginx_access_by_status.3xx",
  "nginx_access_by_status.4xx",
  "nginx_access_by_status.5xx",
  "nginx_access_by_status._unmatched",
]
source = """
# Elasticsearch relies on a @timestamp field

.@timestamp, err = to_float(.msec) * 1000
if err != null {
  .@timestamp = null
}

.@timestamp = to_int(.@timestamp)
if .@timestamp == 0 {
  .@timestamp = null
}

tags = [ string(.tags || "") ?? "", "${ELASTICSEARCH_TAGS}" ]
tags = filter(tags) -> |_index, value| { value != "" }

tags, err = join(tags, ", ")
if err == null && length(tags) != 0 {
  .tags = tags
}
"""

[sinks.elasticsearch_nginx_access]
type = "elasticsearch"
inputs = ["elasticsearch_nginx_access_preprocessed"]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"

  [sinks.elasticsearch_nginx_access.bulk]
  index = "boundary-node-nginx-access-%Y.%m.%d"

  [sinks.elasticsearch_nginx_access.tls]
  verify_certificate = false

# nginx error

[transforms.nginx_error]
type = "filter"
inputs = ["nginx"]
condition = ".SYSLOG_IDENTIFIER == \"error\""

[transforms.nginx_error_clean]
type = "filter"
inputs = ["nginx_error"]
condition = "!contains(string!(.message), \"closed keepalive connection\")"

[transforms.nginx_error_json]
type = "remap"
inputs = ["nginx_error_clean"]
source = """
.@timestamp, err = to_int(.__REALTIME_TIMESTAMP)
if err != null {
  .@timestamp = null
}

.@timestamp, err = .@timestamp / 1000
if err != null {
  .timestamp = null
}
.@timestamp = to_int(.@timestamp)

. = {
  "@timestamp": .@timestamp,
  "host": .host,
  "message": .message
}
"""

# nginx error (metrics)

[transforms.nginx_error_metrics]
type = "log_to_metric"
inputs = ["nginx_error_json"]

  [[transforms.nginx_error_metrics.metrics]]
  type = "counter"
  field = "message"
  name = "error_total"

    [transforms.nginx_error_metrics.metrics.tags]
    hostname = "{{ host }}"

# nginx (prometheus)

[sinks.prometheus_exporter_nginx]
type = "prometheus_exporter"
inputs = ["nginx_access_metrics", "nginx_error_metrics"]
address = "${NGINX_PROMETHUS_ADDR}"
default_namespace = "nginx"
suppress_timestamp = true

# control-plane

[sources.control_plane]
type = "journald"
include_units = ["control-plane"]

[transforms.control_plane_json]
type = "remap"
inputs = ["control_plane"]
source = """
preserved_fields = {}; preserved_keys = ["host"]

for_each(preserved_keys) -> |_, k| {
  v = get!(., [k])
  if v != null {
    preserved_fields = set!(preserved_fields, [k], v)
  }
}

. = parse_json!(.message)

.@timestamp = to_timestamp!(.timestamp, unit: "milliseconds")
.@timestamp = to_unix_timestamp(.@timestamp, unit: "milliseconds")
del(.timestamp)

. = merge(., preserved_fields)
"""

[sinks.elasticsearch_control_plane]
type = "elasticsearch"
inputs = ["control_plane_json"]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"

  [sinks.elasticsearch_control_plane.bulk]
  index = "boundary-node-control-plane-%Y.%m.%d"

  [sinks.elasticsearch_control_plane.tls]
  verify_certificate = false

# certificate-issuer

[sources.certificate_issuer]
type = "journald"
include_units = ["certificate-issuer"]

[transforms.certificate_issuer_json]
type = "remap"
inputs = ["certificate_issuer"]
source = """
preserved_fields = {}; preserved_keys = ["host"]

for_each(preserved_keys) -> |_, k| {
  v = get!(., [k])
  if v != null {
    preserved_fields = set!(preserved_fields, [k], v)
  }
}

. = parse_json!(.message)

.@timestamp = to_timestamp!(.timestamp, unit: "milliseconds")
.@timestamp = to_unix_timestamp(.@timestamp, unit: "milliseconds")
del(.timestamp)

. = merge(., preserved_fields)
"""

[sinks.elasticsearch_certificate_issuer]
type = "elasticsearch"
inputs = ["certificate_issuer_json"]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"

  [sinks.elasticsearch_certificate_issuer.bulk]
  index = "boundary-node-certificate-issuer-%Y.%m.%d"

  [sinks.elasticsearch_certificate_issuer.tls]
  verify_certificate = false

# certificate-syncer

[sources.certificate_syncer]
type = "journald"
include_units = ["certificate-syncer"]

[transforms.certificate_syncer_json]
type = "remap"
inputs = ["certificate_syncer"]
source = """
preserved_fields = {}; preserved_keys = ["host"]

for_each(preserved_keys) -> |_, k| {
  v = get!(., [k])
  if v != null {
    preserved_fields = set!(preserved_fields, [k], v)
  }
}

. = parse_json!(.message)

.@timestamp = to_timestamp!(.timestamp, unit: "milliseconds")
.@timestamp = to_unix_timestamp(.@timestamp, unit: "milliseconds")
del(.timestamp)

. = merge(., preserved_fields)
"""

[sinks.elasticsearch_certificate_syncer]
type = "elasticsearch"
inputs = ["certificate_syncer_json"]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"

  [sinks.elasticsearch_certificate_syncer.bulk]
  index = "boundary-node-certificate-syncer-%Y.%m.%d"

  [sinks.elasticsearch_certificate_syncer.tls]
  verify_certificate = false

# danted (socks proxy)

[sources.danted]
type = "journald"
include_units = ["danted"]

[transforms.danted_json]
type = "remap"
inputs = ["danted"]
source = """
preserved_fields = {}; preserved_keys = ["host", "timestamp"]

for_each(preserved_keys) -> |_, k| {
  v = get!(., [k])
  if v != null {
    preserved_fields = set!(preserved_fields, [k], v)
  }
}

.message = string!(.message)
addrs = split(.message, " [: ")[-1]
addrs = split(string!(addrs), " ")

client_addr_with_port = split(addrs[0], ".") ?? ["N/A", "N/A"]
server_addr_with_port = split(addrs[1], ".") ?? ["N/A", "N/A"]

. = merge({
  "client_addr": client_addr_with_port[0],
  "client_port": client_addr_with_port[1],
  "server_addr": server_addr_with_port[0],
  "server_port": server_addr_with_port[1],
}, preserved_fields)
"""

# danted (socks proxy) (metrics)

[transforms.danted_metrics]
type = "log_to_metric"
inputs = ["danted_json"]

  [[transforms.danted_metrics.metrics]]
  type = "counter"
  field = "timestamp"
  name = "requests_total"

    [transforms.danted_metrics.metrics.tags]
    hostname = "{{ host }}"
    client_addr = "{{ client_addr }}"
    server_addr = "{{ server_addr }}"

# danted (socks proxy) (prometheus)

[sinks.prometheus_exporter_danted]
type = "prometheus_exporter"
inputs = ["danted_metrics"]
address = "${DANTED_PROMETHUS_ADDR}"
default_namespace = "danted"
suppress_timestamp = true

# this metric sees very low activity,
# therefore we retain it for a longer time (2 hrs)
flush_period_secs = 7200
