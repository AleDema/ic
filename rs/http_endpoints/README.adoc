= Public HTTPS Endpoints =
:toc:
 
== Introduction ==
 
The directory contains replica components that listen on network sockets, so NNS managed nodes can 
communicate with each other.

The replica components in this directory are:

* link:public/README.adoc[public HTTPS API endpoint], serving incoming requests from boundary nodes and other replica nodes
* metrics HTTPS endpoint, used by https://prometheus.io/[Prometheus] for scraping

== Connection management ==

Components in scope assume persistent connections are established.

=== Using Nftables ===

The ReplicaOS uses https://en.wikipedia.org/wiki/Nftables[nftables] for setting firewall rules that are important
for the overall reliability of the IC. Nftables are used for:

 *  restrict inbound traffic. Only IPs of nodes that are in the registry can establish connections to IC nodes on whitelisted ports. 
 *  limit the number of simultaneous TCP connections from a source IP. 
 *  limit the rate at which connections are established from a source IP.

The above rules serve as protection against:

 *  protocol attacks by 3rd parties - since only IPs of nodes in the registry have access to replica nodes.
 *  shared resource exhaustion (e.g. file descriptor exhaustion due to many connections, 
 CPU exhaustion due to excessive number of TLS handshakes).

=== Dead peer detection ===

Given the existence of persistent connections, we must detect dead peers, disconnections due
to network inactivity and/or peers holding on to connections without sending requests.
For this purpose, if no bytes are read from a connection for the duration of 
`+connection_read_timeout_seconds+` then the connection is dropped. There is no point in 
setting a timeout on the write bytes since they are conditioned on the received requests. 

Each component uses the ReplicaOS defaults for https://tldp.org/HOWTO/TCP-Keepalive-HOWTO/overview.html#whyuse[TCP alivekeep].

== Thread-per-request ==
 
Components in scope listen on a socket, parse the corresponding request and execute the requested API
call by calling an upstream component(s) that may block the running thread. Since async code needs to be
https://docs.rs/tokio/latest/tokio/task/index.html[non-blocking], the components use 
https://sre.google/sre-book/addressing-cascading-failures/#xref_cascading-failure_queue-management[thread-per-request]
pattern for executing the corresponding upstream API calls. More specifically, there is a
queue in front of each thread pool that handles requests. Requests come in, they sit in a bounded-size queue, and then
threads pick requests off the queue and perform the actual work (whatever actions are required by the replica).
The caller is responsible for correctly handling the case when the queue is full.

If a request is cancelled before it is picked up from a thread, then the request is not executed.

This design pattern is implemented using a https://docs.rs/tower/latest/tower/limit/concurrency/index.html[limit on number of requests being concurrently processed],
https://docs.rs/threadpool/latest/threadpool/[threadpool] per upstream service and https://docs.rs/tokio/latest/tokio/sync/oneshot/index.html[Tokio oneshot channel].

== Request timeout ==

In order to guard against stuck upstream services, a https://docs.rs/tower/latest/tower/timeout/index.html[timeout] is set for each received request. 
If a request is not completed within the timeout then the endpoint responds with `+504 Gateway Timeout+`.

Setting a timeout on requests also guards against closing an idle connection due to stuck upstream service.
For example, if a user uses HTTP1 over a connection and there is a upstream service that takes longer
than it should, then the user is unable to send new requests until the last one completed.
Hence, this may result in dropping the connection because no new bytes are read by the replica on that
connection. 

== Preventing Server Overload ==
 
Servers should protect themselves from becoming overloaded and crashing. When overloaded at either the frontend or
backend layers, fail early and cheaply. For details, see 
https://sre.google/sre-book/addressing-cascading-failures/#xref_cascading-failure_load-shed-graceful-degredation[Load Shedding and Graceful Degradation.]

In addition, serving errors early and cheaply can be beneficial for replicated servers that stay behind load balancers.
For example, https://sre.google/sre-book/load-balancing-datacenter/[Least-Loaded Round Robin] takes into account recent errors.
 
Given the listed best practices, when a particular queue in-front of a thread pool is full and request can't be added,
the endpoint https://docs.rs/tower/latest/tower/load_shed/index.html#[sheds load] by responding with `+429 Too Many Requests+` for that particular request. 

== Too large and too slow requests ==

If a http request body is greater than the configured limit, the endpoints responds with `+413 Payload Too Large+`.

If a http request does not complete within the specified timeout it will be aborted and a `+408 Request Timeout+` response will be sent.

== Fairness ==

Fairness implies that the probability of picking up a request from any connection is uniformly distributed.

Currently boundary nodes use https://www.nginx.com/[nginx] as reverse proxy. Thus, the boundary nodes
can only use https://mailman.nginx.org/pipermail/nginx/2015-December/049445.html[HTTP1] for communicating with replicas.
Hence, there can be at most one in-flight request in the replica from each connection. Given we use https://tokio.rs/blog/2019-10-scheduler[Tokio's fair scheduler]
and we have a dedicated Tokio task for serving each connection, this is sufficient to provide request fairness across connections.

== Graceful shutdown ==

All the components are written in Rust async, implementing at least one event loop that lives for the duration
of the replica process. For graceful shutdown of such event loops the replica relies on the Tokio runtime to cancel the 
running and queued futures. Such cancellation happens when the runtime is dropped.

== HTTP vs HTTPS ==

Each component has a single listening port that accepts a TCP connection. After a connection is accepted, 
the component peeks the first byte of the TCP stream to determine if a TLS connection is initiated.
